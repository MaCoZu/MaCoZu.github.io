# Normal Distribution
All kinds of variables in natural and social sciences are (approximately) normally distributed. Height, birth weight, reading ability, job satisfaction, or test scores are just a few examples of such variables. Because normally distributed variables are so common, many statistical tests are designed around them.

Normal distributions can be applied even to variables that are not normally distributed, with the help of the **central limit theorem (CLT).** The CLT says that when you repeatedly sample from independent and identically distributed (other than normally distributed) random variables, and take the arithmetic mean of each of those sample, these means will create a distribution that closely approximates a normal distribution. Thereby statistical methods that work for normal distributions can be applied to many problems involving other types of distributions.

In a normal distribution the observations are clustered around the mean and become sparse as they move away from the center. This leads to the typical bell-shaped curve. The normal distribution is often identified as the bell curve but there is other bell-shaped distributions. Assuming that the observations fall at the same rate above and below the mean the normal distribution is symmetric which yields convenient characteristics. For example the measures of central tendency – mean ($\mu$), median, and mode – fall in one place. This place splits the dataset cleanly in the middle where 50% of the values fall below and the other half above the mean. 

The distribution can be described by two values: the mean and the standard deviation. While the mean determines the location of the normal distribution on the $x$-axis the standard deviation defines its shape. The standard deviation $\sigma$ can be thought of as a “typical” distance of the observations $x_i$ from their mean, $\mu$. Higher standard deviations make for flatter curves, try it by moving the slider of the graph to a value of $\sigma>1$. 

A further very practical feature of normal distributions is called the empirical rule (aka. ${68-95-99.7}$ rule) and it states, that 68% of the observations lie within one standard deviation ($\pm \, \sigma$) around the mean, 95% of observation within $\pm \, 2\sigma$, and 97% within $\pm \, 3\sigma$. The empirical rule is not affected by different values of standard deviation, that is to say that a flat normal distribution ($\sigma>1$) and a peaked normal distribution ($\sigma<1$) both adhere to the rule. Say, you have a normal distribution with $\mu=100$ and $\sigma=15$ than you can immediately tell that 95% of you observations will be found in the range of $[70, 130]$ or $[100 - 2\times15, 100 + 2\times15]$. 

In business companies try to reduce the deviation from a desired property to a minimum. For example their goal may be to improve their production or business process so that any deviation from the perfect quality becomes negligibly small. Motorola coined the **Six Sigma** method, where rigorous quality control reduces aberrations to such a minuscule $\sigma$, that even output that strays six standard deviations ($\pm\,6\sigma$) away from the optimum quality, can be rendered good enough. In other words: the production process becomes so precise, that six sigma's of deviation are not a problem.

The usual application of the normal distribution can be summarized in the question: “How likely is it that I will see an observation smaller, equal to or greater than some value $x$?” To simplify things one often standardizes the $x$ in question into the corresponding $z$-value, by subtracting the mean and dividing with the standard deviation. This makes it easy to look up the probability in a prepared table for the standard normal distribution.
$$
z=(\frac{{x - \mu}}{{\sigma}})
$$
 The standard normal distribution is the normal distribution with the parameters $\mu=0$ and $\sigma=1$. Standardization is handy because any normal distribution can be transformed into a standard normal distribution. In fact every normal distribution is a version of the standard normal distribution, whose domain has been stretched by $\sigma$ and shifted by $\mu$. Thus, any $x$-value from one normal distributions, can be transformed in its corresponding $z$-value and be checked on the same table as any other $x$-value coming from a different normal distribution (with different $\mu$ and $\sigma$ parameters).

By playing with the graph above your essentially transforming the $x$-values of the curve through the parameters, if you put the sliders to $\mu=0$ and $\sigma=1$, you’ll see the standard normal distribution. The transition from some other shape of the curve to this one can be understood as the standardization of all the $x$-values. 

Normal Distribution (Probability) Function: 
$$
f(x|\mu, \sigma) = \frac{1}{{\sigma \sqrt{2\pi}}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$
Standard Normal (Probability) Function: 
$$
f(z) =\frac{e^{-\frac{z^2}{2}}}{\sqrt{2\pi}}
$$

For example if we have a data set with a mean $\mu_x = 5.0$ and standard deviation $\sigma_x=3.09$, $z$-scores are as follows:

<table class="float-right w-auto border-collapse border border-gray-300 text-sm">
  <thead>
    <tr class="bg-gray-100">
      <th style="border: 1px solid #ccc; padding: 2px 1px; text-align: center; font-semibold">x</th>
      <th style="border: 1px solid #ccc; padding: 2px 1px; text-align: center; font-semibold">z</th>
      <th style="border: 1px solid #ccc; padding: 2px 1px; text-align: center; font-semibold">p-value</th>
    </tr>
  </thead>
  <tbody>
    <tr class="bg-gray-50">
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">-1.62</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.053</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">2</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">-0.97</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.166</td>
    </tr>
    <tr class="bg-gray-50">
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">2</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">-0.97</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.166</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">5</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.5</td>
    </tr>
    <tr class="bg-gray-50">
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">5</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.5</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">5</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.5</td>
    </tr>
    <tr class="bg-gray-50">
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">8</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.97</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.83</td>
    </tr>
    <tr>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">8</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.97</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.83</td>
    </tr>
    <tr class="bg-gray-50">
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">10</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">1.62</td>
      <td style="border: 1px solid #ccc; padding: 2px 1px; text-align: center;">0.95</td>
    </tr>
  </tbody>
</table>

Each data point $x$ is now expressed as the difference from $\mu_x$ in units of the standard deviation $\sigma_x$, this is the $z$-score.

Each $z$-score is associated with a probability, or $\mathbf{p}$**-value**, that tells you the cumulative probability of values occurring below that $z$-score. That is the percentage of observations that will likely occur up to this point. You could for example ask, how likely is it, that I will see an observation equal to or below 8: 
$$
P(x\leq8)=P(z\leq0.97)=0.83\%
$$

With the p-values you can also determine how likely it is, that random variable in the distribution is greater than 10. Since the $p$-value typically gives the probability of values occurring below the $x$ in question, you will find the probability of values occurring above this point by subtracting the $p$-value from 1:

$$
P(x\leq10)=P(z\leq1.62) =0.95\% \rightarrow (1-0.95)=0.05\%
$$


If you ask for the probability of some $x$ occurring in a certain range of values, you subtract the smaller cumulative probability from the greater cumulative probability. Since the greater probability already includes the case of the smaller one. Here the probability of $x<10$ already includes the case of $x<2$, if we wanted to know the probability of $2<x<10$ you have to remove the smaller probability from the greater. 
$$
P(2<x<10) = (0.95 - 0.166)=0.78\%
$$  

If you had mutually exclusive cases you would add the probabilities together. 
$$
P(2<x) \,or\, P(x>10) = 0.166 + 0.05 =0.174\%
$$