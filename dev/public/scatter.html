<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.14.0/dist/cdn.min.js"></script>

  <!-- Favicon -->
  <link rel="icon" href="img/logo/logo_weiß.svg" type="image/x-icon" />

  <style>
    /* Apply smooth scrolling behavior */
    /* .smooth-scroll {
            scroll-behavior: smooth;
        } */

    [x-cloak] {
      display: none !important;
    }

    /* MENU */
    .font-pop {
      font-family: 'Poppins', sans-serif;
    }

    /* TEXT */
    .font-literata {
      font-family: 'Literata', serif;
    }

    /* HIGHLIGHT  */
    .font-bebas {
      font-family: 'Bebas', sans-serif;
    }

    .font-roboto {
      font-family: 'Roboto', normal;
    }

    .font-merri {
      font-family: 'Merriweather', serif;
    }

    .font-pop {
      font-family: 'Poppins', sans-serif;
    }

    .font-lora {
      font-family: 'Lora', sans-serif;
    }

    .font-ledger {
      font-family: 'Ledger', sans-serif;
    }

    .font-signika {
      font-family: 'Signika', sans-serif;
    }

    /* New styles for sticky navbar */
    .sticky-nav {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      z-index: 1000;
    }
  </style>

  <!-- Fonts -->
  <link rel="stylesheet" href="../font.css">

  <!-- css  -->
  <link rel="stylesheet" href="../style.css">

  <!-- mathjax  -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        messageStyle: "normal",
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
            }
        });
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"></script>


  <title>Scatter to Regression</title>

</head>

<body class="font-pop bg-white mx-auto md:mx-20 px-6 md:px-0 text-gray-700 dark:bg-gray-900 dark:text-gray-200">

  <!-- new nav bar  -->
  <nav x-data="{ isOpen: false, showSearch: false }"
    class="sticky-nav sticky md:relative top-0 left-0 right-0 dark:bg-gray-900 dark:text-gray-200 z-50"
    @click.away="isOpen = false">
    <div class="container px-4 py-2 md:py-3 mx-auto">
      <div class="flex items-center justify-between">
        <!-- Logo -->
        <a href="index.html" class="flex items-center">
          <img id="logo" class="h-8 sm:h-8 fill-current dark:filter-white" src="./img/logo/logo.svg" alt="logo">
        </a>
        <!-- Mobile menu button -->
        <div class="md:hidden">
          <button @click="isOpen = !isOpen"
            class="p-2 text-gray-400 rounded-md hover:text-indigo-500 focus:outline-none focus:text-gray-200"
            aria-label="toggle menu">
            <svg x-show="!isOpen" xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24"
              stroke="currentColor" stroke-width="2">
              <path stroke-linecap="round" stroke-linejoin="round" d="M4 6h16M4 12h16M4 18h16" />
            </svg>
            <svg x-show="isOpen" xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24"
              stroke="currentColor" stroke-width="2">
              <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
            </svg>
          </button>
        </div>
        <!-- Desktop Menu -->
        <div class="hidden md:flex md:items-center">
          <div class="flex flex-row md:mx-6">
            <a class="text-sm font-medium hover:text-indigo-500 md:mx-4 md:my-0" href="index.html">Home</a>
            <a class="text-sm font-medium hover:text-indigo-500 md:mx-4 md:my-0" href="impressum.html">Impressum</a>
          </div>
        </div>
      </div>

      <!-- Mobile Menu (Dropdown) -->
      <div x-show="isOpen"
        class="absolute right-0 mt-2 py-2 w-56 dark:bg-gray-800 dark:text-gray-200 rounded-md shadow-xl z-20"
        x-transition:enter="transition ease-out duration-200" x-transition:enter-start="opacity-0 transform scale-90"
        x-transition:enter-end="opacity-100 transform scale-100" x-transition:leave="transition ease-in duration-200"
        x-transition:leave-start="opacity-100 transform scale-100"
        x-transition:leave-end="opacity-0 transform scale-90">

        <a href="#intro" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Introduction</a>
        <a href="#terminology" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Terminology</a>
        <a href="#scatterplot" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">The Scatterplot</a>
        <a href="#spread" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Spread</a>
        <a href="#averages" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Averages</a>
        <a href="#goa" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Point &amp; Graph of
          Averages</a>
        <a href="#sdline" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Standard Deviation
          Line</a>
        <a href="#correlation" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Pearson's
          Correlation</a>
        <a href="#regression_method" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Regression
          Method</a>
        <a href="#OLS" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Method of Least Squares</a>
        <a href="#interpretation" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Interpretation</a>
        <a href="#summary" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Summary</a>
        <a href="#sources" class="block px-4 py-1 text-sm text-gray-200 hover:text-indigo-500">Sources</a>

        <hr class="my-2 mx-auto w-40 self-center solid z-40">

        <a href="index.html" class="block px-4 py-2 text-sm text-gray-200 hover:text-indigo-500">Home</a>
        <a href="impressum.html" class="block px-4 py-2 text-sm text-gray-200 hover:text-indigo-500">Impressum</a>
      </div>
    </div>
  </nav>

  <aside
    class="hidden md:block right-0 w-1/6 py-8 fixed top-1/2 transform -translate-y-1/2 flex-col items-center overflow-y-auto bg-white dark:bg-gray-900 dark:border-gray-700">
    <nav class="flex flex-col flex-1 space-y-8 items-center justify-center">
      <a type="button" id="chevron-up">
        <span
          class="icon-[heroicons-outline--chevron-up] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
      </a>

      <a type="button" id="chevron-down">
        <span
          class="icon-[heroicons-outline--chevron-down] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
      </a>

      <a class=""></a>

      <a id="font-select" data-tooltip-placement="left">
        <span
          class="icon-[fa6-solid--font] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
      </a>

      <a class=""></a>

      <a type="button" href="#top" id="chevron-up-double">
        <span
          class="icon-[heroicons-outline--chevron-double-up] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
      </a>

      <a type="button" href="#footer" id="chevron-down-double">
        <span
          class="icon-[heroicons-outline--chevron-double-down] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
      </a>

  </aside>

  <!-- drawer  -->
  <div class="hidden md:flex z-50">
    <div id="drawer-toggle"
      class="fixed z-30 top-1/2 right-0 inline-block p-2 transition-all duration-200 rounded-lg cursor-pointer">
      <svg class="w-10 h-10 md:w-full md:h-full dark:text-white" width="50" height="100" viewBox="0 0 50 100"
        xmlns="http://www.w3.org/2000/svg">
        <path d="M10 10 H40 M10 20 H40 M10 30 H40" style="stroke: #CCC; fill:none; stroke-width: 4px"
          stroke-opacity="0.7" stroke-linecap="round">
        </path>
      </svg>
    </div>

    <div id="sidebar_model"
      class="fixed top-0 right-0 z-50 w-64 h-full transition-all duration-500 transform translate-x-full bg-white dark:bg-gray-800 shadow-lg">
      <div class="px-6 py-4">
        <div class="pr-6 py-6 flex justify-between items-center">
          <h1 class="text-3xl font-bebas font-semibold">Contents</h1>
          <!-- <div id="drawer-close" class="cursor-pointer">
                      <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6 text-black dark:text-white" fill="none"
                          viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                          <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
                      </svg>
                  </div> -->
        </div>
        <ul class="text-lg font-roboto mt-4">
          <li class="mt-1"><a href="#intro" class="block hover:text-indigo-400">Introduction</a></li>
          <li class="mt-1"><a href="#terminology" class="block hover:text-indigo-400">Terminology</a></li>
          <li class="mt-1"><a href="#scatterplot" class="block hover:text-indigo-400">The Scatterplot</a></li>
          <li class="mt-1"><a href="#spread" class="block hover:text-indigo-400">Spread</a></li>
          <li class="mt-1"><a href="#averages" class="block hover:text-indigo-400">Averages</a></li>
          <li class="mt-1"><a href="#goa" class="block hover:text-indigo-400">Point &amp; Graph of Averages</a></li>
          <li class="mt-1"><a href="#sdline" class="block hover:text-indigo-400">Standard Deviation Line</a></li>
          <li class="mt-1"><a href="#correlation" class="block hover:text-indigo-400">Pearson's Correlation</a></li>
          <li class="mt-1"><a href="#regression_method" class="block hover:text-indigo-400">Regression Method</a></li>
          <li class="mt-1"><a href="#OLS" class="block hover:text-indigo-400">Method of Least Squares</a></li>
          <li class="mt-1"><a href="#interpretation" class="block hover:text-indigo-400">Interpretation</a></li>
          <li class="mt-1"><a href="#summary" class="block hover:text-indigo-400">Summary</a></li>
          <li class="mt-1"><a href="#sources" class="block hover:text-indigo-400">Sources</a></li>
        </ul>
        <div class="text-xs fixed bottom-10 flex row">
          <a href="index.html" class="flex items-center">
            <img id="logo2" class="h-8 sm:h-8 fill-current dark:filter-white" src="./img/logo/logo.svg" alt="logo">
          </a>
        </div>
      </div>
    </div>
  </div>

  <!-- Main content -->
  <main class="flex justify-center items-center min-h-screen my-20 px-0">
    <article id="article"
      class="w-5/6 font-literata md:w-1/2 max-w-full prose md:prose-xl prose-stone text-pretty leading-10 dark:prose-invert">

      <!-- title  -->
      <h1 class="text-left leading-snug">From Scatterplot to Regression Line</h1>

      <!-- intro -->
      <section>
        <!-- table of contents  -->
        <div class="float-none md:float-right md:mt-8">
          <div class="border rounded-lg w-fit pl-4 pr-4 pb-0 pt-0 ml-6 mt-2 mb-1 shadow-sm z-40">
            <p class="text-lg font-bold text-left border-b border-gray-300">Contents</p>
            <ul class="text-base list-none text-left pl-0 ">
              <li><a href="#intro">Introduction</a>
              <li><a href="#terminology">Terminology</a>
              <li><a href="#scatterplot">The Scatterplot</a>
              <li><a href="#spread">Spread</a>
              <li><a href="#averages">Averages</a>
              <li><a href="#goa">Point &amp; Graph of Averages</a>
              <li><a href="#sdline">Standard Deviation Line</a>
              <li><a href="#correlation">Pearson's Correlation</a>
              <li><a href="#regression_method">Regression Method</a>
              <li><a href="#OLS">Method of Least Squares</a>
              <li><a href="#interpretation">Interpretation</a>
              <li><a href="#summary">Summary</a>
              <li><a href="#sources">Sources</a>
            </ul>
          </div>
        </div>
        <h2 id="intro" class="scroll-mt-16 md:scroll-mt-10">Intro</h2>
        <p>Techniques of inference or prediction have become an integral part of modern life, from weather forecasting,
          traffic management systems, economic projections and the constant monitoring of smart devices, statistics is
          not just observing our behavior it’s driving it. I think its a good idea to have some understanding of the
          nuts and bolts of the machinery behind this process. </p>
        <p>We’ll develop a simple linear regression step by step as it is an excellent introduction to the topic of
          “statistical inference”. A host of smaller statistical ideas reach a first point of culmination in a simple
          regression line, and hopefully you’ll see how even the most intricate machine learning model is build on those
          basic components.</p>
      </section>

      <!-- terminology  -->
      <section>
        <h2 id="terminology" class="scroll-mt-16 md:scroll-mt-10">Terminology</h2>
        <p>Mathematics is a language in itself, it may be precise but not always unambiguous. So let’s clarify some
          terms first.</p>
        <p>Regression is a branch of <strong>statistical inference</strong>, which refers to the process of drawing
          conclusions about a population based on a sample of that population. Linear regression is a
          <strong>frequentist inference</strong> method based on the frequency of events in repeated experiments.
        </p>
        <p>Frequentism can be contrasted with <strong>theoretical probability</strong> – that is purely mathematical
          like the probability of getting tails when flipping a coin – or <strong>Bayesianism</strong> which
          incorporates subjective beliefs about the probability of an event and updates prior beliefs with observed
          data.</p>
        <p><strong>Prediction</strong> is often used interchangeably with <strong>forecast</strong>, but the words have
          different meanings in certain fields. Forecast specifically refers to future events while predictions can be
          about past, present, or future events. And then there is different interpretations of theses terms. For
          example in seismology a prediction of an earthquake specifies some exact future time and place, whereas a
          forecast is more vague and only states the likelihood of an earthquake in a certain region over a certain
          period of time. In demography and climate science, the term <strong>projection</strong>, refers to a future
          development under certain conditions (<strong>scenario</strong>). </p>
        <p>I will use the term prediction throughout the text, since we’re not concern with the future for now.
          Furthermore, simple regression is not apt to look far in the future, this may require complex factors like
          seasonality, trends, and external variables. Thus, with simple regression it is generally a good idea to stay
          close to your observed data beyond which predictions quickly looses their significance.</p>
        <p><strong>Simple regression</strong> describes the relationship between an <em>independent</em> (explanatory)
          variable and a single <em>dependent variable</em> (response). The relationship is (often) assumed to be
          linear, and the equation of a <em>simple linear regression</em> is: $$y=β_0+β_1x+ε \tag{1}$$<br />
          Where: <br />
          $y=$ response, the quantity to be estimated or modeled.<br />
          $x=$ explanatory variable, the known input to the model, used as a predictor of $y$.<br />
          $\beta_0$ (beta zero) $=$ $y$-<strong>intercept</strong> of the line, that is, the point at which the line
          intercepts or cuts through the $y$-axis<br />
          $\beta_1$ (beta one) = <strong>Slope</strong> of the line, i.e. the change (amount of increase or decrease) of
          $y$ for every unit increase in $x$.<br />
          $\varepsilon=$ Random error component, adds an allowance for error in the prediction.</p>
        <p>By the end of this text, we'll arrive at an equation that has little hats on some symbols, indicating that
          the symbols below them are <strong>estimates</strong> of the underlying but unknown parameters. The population
          parameters thereby become so-called <strong>sample statistics</strong>.<br />
          $$\hat{y}=\hat\beta_0+\hat\beta_{1}x \tag{2}$$ In this equation, the error term $ε$ is missing, because by
          finding the estimates of the parameters from equation $(1)$ we specifically minimized the errors, so that the
          sum of errors becomes zero $\sum ε_i =0$. But estimators are not the real population parameters and the
          remaining uncertainty is signified by little hats above our sample statistics ($\hat\beta_0, \hat\beta_{1})$.
        </p>
        <p><strong>Multiple linear regression</strong> extends simple regression to include many explanatory variables
          and aims to estimate the linear relationship between these predictors and a single response.<br />
          $$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_kx_k + ε$$</p>
        <p>There are many synonyms for the independent and dependent variables. I have introduced you to
          <strong>explanatory and response variable</strong>, which I find most expressive because the explanatory
          variable <em>explains</em> how the other variable <em>responds</em>. Also, the e$x$planatory has the symbol
          $x$ in it, which denotes its variables in the formula.
        </p>
        <p>The explanatory and response variables are <strong>characteristics</strong> of
          <strong>observational units</strong> (person, thing, transaction, or event), that we measured,
          observed or polled from a <strong>sample</strong> (subset) of a <strong>population</strong>
          (entirety of observational units of interest).
        </p>
        <p>In statistics we're almost always dealing with a sample because we rarely get information from
          the entire population. <strong>Inferring</strong> something from the known (sample) about the
          unknown (the rest of the population) is the main reason for doing statistics in the first place.
        </p>
        <p>Since we’re working with samples, we arrive at <strong>sample “statistics”</strong> which are approximations
          of the true but unknown <strong>population “parameters”</strong>. Sample statistics are often computed by
          dividing them by $(n-1)$ instead of just dividing by $n$. For example the <em>unbiased</em> sample variance is
          calculated with:<br />
          $$s^{2}= \frac{1}{n-1}\sum\limits_{i=1}(x_i-\bar{x})$$<br />
          Dividing sample statistics by $(n-1)$ is called <strong>Bessel's correction</strong>. The idea is that samples
          of data seldom represent the whole spectrum of variability in the population, by dividing a little less the
          variance becomes a bit bigger and hopefully more realistic.</p>

        <p><strong>Sampling aka. Data collection</strong> is of utmost importance for statistics and most of what’s
          wrong with statistics comes from error-prone data collection. A good representative sample should give any
          characteristic in a population or any strata of society an equal change of inclusion in the sample by
          randomized selection and avoiding bias. Which for many reasons is often not realized. </p>
        <p>Infamous examples of biased selection include:</p>
        <ul>
          <li>Surveying only Western college students known as <strong>WEIRD</strong> (<strong>W</strong>estern,
            <strong>E</strong>ducated, <strong>I</strong>ndustrialized, <strong>R</strong>ich and
            <strong>D</strong>emocratic) people for <a
              href="https://www.scientificamerican.com/podcast/episode/psychology-studies-biased-toward-we-10-08-07/">psychological
              studies</a> and attributing the findings to the broader population or even all of humanity. Also called
            <em>volunteer bias</em>.
          </li>
          <li><em>Publication bias</em>, where academic research is only published if significant findings are made.
            Shifting publications in favor of positive results.</li>
          <li>Or when the media neglects journalistic standards of neutrality and objectivity, in favor of certain
            worldviews and cultures, or worse, <a href="https://en.wikipedia.org/wiki/Manufacturing_Consent">pandering
              to particular ideologies</a>, aka <em>media bias</em>. </li>
        </ul>
        <p>Terminology can get messy when we talk about machine learning, as old concepts get new names to
          better fit the context. Here are the main vocabulary shifts between statistics and machine
          learning. </p>
        <ul>
          <li>A sample becomes a <strong>dataset</strong>.</li>
          <li>Explanatory (independent) variables ($x$) become <strong>features</strong>. </li>
          <li>Predictor (dependent) variable ($y$) becomes <strong>target variable</strong> or
            <strong>label</strong>.
          </li>
        </ul>
        <p><strong>After we have built a model</strong> and put it into practice, we feed an
          <strong>input</strong> $x$ to the model and expect an <strong>output</strong> – an estimate of $y$ –
          denoted by
          $\hat{y}$.
        </p>
        <p>To evaluate the quality of the model, we can compare the estimate $\hat{y}$ with the <strong>true
            label</strong> $y$. The difference between the true label and the output is called the
          <strong>error</strong>.
        </p>
        <p>$$error=(y-\hat{y})$$</p>
        <p>Similar but not to be confused with the errors are <strong>deviations</strong>. A deviation is
          the difference between an observed value and a measure of central tendency (such as the mean or
          median). Deviations are typically used to understand the <strong>variability</strong> within a
          dataset.</p>
        <p>$$deviation=(y-\bar{y})$$ Now we’re good to go.</p>
      </section>

      <!-- scatterplot  -->
      <section>
        <h2 id="scatterplot" class="scroll-mt-16 md:scroll-mt-10">The Scatterplot</h2>
        <p>In the beginning there was the scatterplot. When Sir Francis Galton (1822 – 1911) plotted the height of
          parents against the height of their adult children, he noticed the tendency for tall parents to have tall
          children, but the children were not as tall, on average, as their parents. The same was true for short
          parents, their children were generally shorter but not as short as them, on average. So while the height of
          parents and their children is correlated (behaves similar), the extreme characteristics of parents are not
          amplified by their children but dampened. Galton called this phenomenon “regression toward mediocrity”.
          Looking at Galton’s data we can see how the average heights of adult children tend to <em>regress</em> to the
          population average. The $45°$ Line suggests a $1:1$ association in the height of children and parents, in
          contrast the Regression Line, derived from the data, shows a weaker correlation and reflects the regress of
          extremes.</p>
        <figure class="flex flex-col justify-center items-center">
          <div class="w-[80%] hover:scale-125">
            <img src="img/scatter_reg/galton1.png" alt="Correlation coefficients">
          </div>
          <figcaption class="mx-5 mt-10 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
            The regression line indicates a weaker than $1:1$ relationship between the average
            height of parents and their offspring.
          </figcaption>
        </figure>

        <p>The graph looks the same 100 years later. Even if people have become taller in general, the proportions stay
          the same. If extremes did not sand off and children developed the characteristics of their parents further and
          further, we’d live in a world of dwarfs and giants, and many other oddities.</p>
        <p>From Galton's discovery we have learned that: </p>
        <ul>
          <li>Repeated measurements fluctuate around a true mean.</li>
          <li>Extremes are do not survive.</li>
        </ul>
        <p>Just looking at a scatterplot the practiced eye can see different things. For example the <em>range</em> of
          values, the <em>spread</em> of observations, the <em>single, and joint mean</em> of the variables, and maybe
          even some characteristics of the relationship between them like <em>direction</em> and <em>strength</em>
          (tight or dispersed data points), and degree (linear, non-linear).</p>
        <p>Let us quickly determine these things quickly to understand how the regression line is
          derived from them. </p>
      </section>

      <!-- spread  -->
      <section>
        <h2 id="spread" class="scroll-mt-16 md:scroll-mt-10">Spread</h2>
        <p>Spread is a measure of variability of the observations around the center, usually the arithmetic mean.</p>
        <p>The simplest measure of spread is the <strong>range</strong>, which is the largest measurement minus the
          smallest. With the range we know which numbers we’re talking about, but we get no sense of what is going on
          between the boundaries.</p>
        <p>The sample <strong>variance</strong> is more useful in that regard. As a measure of dispersion it gives us a
          sense of how far a set of numbers is spread out from their average value. A small variance indicates that most
          data points are clustered around the average value while high variance is a sign of data that is strewn around
          widely. $$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2$$By squaring deviations become absolute values,
          thereby they do not cancel each other out, and big differences get amplified. </p>
        <p>However, squaring the deviations leads to odd units, if we work with $\$$ we get $\$^2$. By taking the square
          root of the variance we arrive at the <strong>standard deviation $s$</strong>, which inherits the advantages
          of the variance but has comprehensible units. $$s=\sqrt{s^2}$$The standard deviation can be thought of as a
          “typical” distance of the observations $x_i$ from their mean, $\bar{x}$. </p>
      </section>

      <!-- averages  -->
      <section>
        <h2 id="averages" class="scroll-mt-16 md:scroll-mt-10">Averages</h2>
        <p>I have suggested the (arithmetic) mean as the point around which our data is spread. But there
          are other "measures of central tendency" to identify the center of a sample. </p>
        <p>For example, the <strong>mode</strong>, which is the most frequent value, or the
          <strong>median</strong>, which is the middle number in a set of ordered (ascending or
          descending) values, and divides the data set in half, separating the lowest 50% from the highest
          50% of values.
        </p>
        <p>The median is considered a <strong>robust</strong> statistic because it is only concerned with
          the values in the middle of the distribution. This means that extreme values do not affect the
          median.</p>
        <p>However, the arithmetic <strong>mean</strong> is the most commonly used measure of central
          tendency. It is not robust because it considers all the values in a sample. This makes the mean
          <strong>sensitive to outliers</strong>, but also makes it a more comprehensive statistic.
          Distributions with some extreme values are said to be <strong>skewed</strong> to the right/left
          or to have a right/left tail. For these distributions a robust measure is more appropriate, but
          for symmetric distributions the mean has some advantages.
        </p>
        <p><strong>Noteworthy advantages of the arithmetic mean</strong>:</p>
        <ul>
          <li>The sum of the deviations of each data point from the mean is always zero. In other words,
            the mean acts as a balancing point for the data distribution, with positive and negative
            deviations canceling each other out.</li>
          <li>The balancing feature of the arithmetic mean also minimizes the sum of the squared deviations:
            $\sum (x_i - \bar{x})^2$. </li>
        </ul>
        <p>Minimizing deviations is at the heart of regression analysis and we’ll see why that is in the
          following.</p>
      </section>

      <!-- point & graph of averages  -->
      <section>
        <h2 id="goa" class="scroll-mt-16 md:scroll-mt-10">Point &amp; Graph of Averages</h2>
        <p>In regression we’re interested in the joint behavior of two and more variables. In the case of
          two variables we want to know how the explanatory variable $x$ describes the response $y$, or at
          least we try to find out if there is a meaningful relationship between them. </p>
        <p>But even without a model we can construct a fine graphical description of the relationship and
          even use it to make predictions. </p>
        <p>The roughest estimate of the joint behavior of both variables can be found in the <strong>point
            of averages</strong> ($\bar{x}, \bar{y}$). This would lead to a statement like "Parents with
          an average height of 67 inches have children with an average height of 66.5 inches". Not very
          expressive, but if we want to be more precise, we could look at a certain section of the sample,
          say parents between 62 and 64 inches and ask about the height of their children. This will give
          us a <strong>group mean</strong> for the average height of children with parents in that range:
          $\bar{y}_{62-64}$.</p>
        <p>Now we can slice the sample across the entire range and calculate the different group means
          $\bar{y}_i$. Connecting these group means gives us a reasonable precursor to the regression
          line, the <strong>graph of averages</strong>. This graph can be tweaked to capture more or less
          variance in the data simply by making the groups narrower or wider and calculating the
          respective group means. Try it out in the interactive plot below.</p>


        <!-- graph of averages  -->
        <figure class="flex flex-col justify-center items-center">
          <div>
            <iframe src="plots/scatter_graphs/graph_of_avg.html" title="Simple Scatter Plot" height="500" width="700"
              scrolling="no" allowfullscreen loading="lazy" seamless></iframe>
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
            More and narrower groups depict more of the variability in the data while wider groups are less
            affected by the samples peculiarities.
          </figcaption>
        </figure>

        <p>The ability to reproduce more or less of the variance found in the data is an important decision
          when building a statistical model, because you want the model to behave realistically in
          response to the inputs, but at the same time the outputs of your model should not reflect all
          the peculiarities of the sample data. In machine learning, this is called
          <strong>generalization</strong>, and the decision between reproducing more or less of the
          information used to build the model involves a <strong>bias-variance tradeoff</strong>.
        </p>
        <p>While the graph of averages results in a fine linear description of the data, it remains a series
          of dots connected by a line. A functional description of the graph of averages, however, would
          require piecemeal linear interpolation along the $x$-segments of the graph. </p>
        <p>The classical regression model, however, is a <strong>parametric model</strong>&nbsp;<input type="checkbox"
            id="cb1" /><label for="cb1"><sup></sup></label><span><br><br>Parametric models
            are described by
            a finite number of parameters and a set of assumptions about the data. As a result, they require
            less data and less time to build a reasonably accurate model, they are easier to understand, and
            the results are interpretable, a property not to be underestimated. But this efficiency comes at
            the cost of some limitations. While finite parameters and a certain set of assumptions simplify
            the problem, parametric models struggle with complex patterns and nonlinearity. Non-parametric
            methods, on the other hand, make no explicit assumptions about the functional form of the
            relationship. Instead, they rely on the data itself to determine the structure of the
            relationship of the variables.<br><br></span>, that is, a
          model based on a finite number of parameters. </p>
      </section>

      <!-- sd line  -->
      <section>
        <h2 id="sdline" class="scroll-mt-16 md:scroll-mt-10">Standard Deviation Line</h2>
        <p>Now we have the pieces to construct a first functional description of the data. With the point of
          averages, indicating the center of the data points and the standard deviations $s_{x}$ and $s_y$
          of both dimensions, we’re able to construct a linear relationship between the two variables,
          that is specified by a few parameters. </p>
        <p>The <strong>Standard Deviation Line</strong> (SD-Line) originates from the point of averages and
          extends in the direction described by the vector between the standard deviations. Suppose we
          have a $s_{x}=1$ and $s_y=2$, then the SD-Line starts at ($\bar{x}, \bar{y}$) and has a slope of
          $\frac{s_y}{s_x}=2$, resulting in a slanted line, showing a positive relationship. </p>

        <figure class="flex justify-center items-center">
          <div>
            <img src="img/scatter_reg/sd_new.png" alt="SD Line">
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

          </figcaption>
        </figure>

        <p>But there’s a catch. The SD-Line would always show a positive relationship because the standard
          deviations (as the variance) are always positive. In calculating the standard deviations, we
          have lost the information that indicates the direction of the deviations. </p>
        <p>Furthermore the standard deviations represent the total variability of a single variable
          calculated independently from each other. The slope of the SD-Line simply contrasts the two
          aggregate statistics $s_{x}$ and $s_y$.</p>
        <p>The <strong>correlation coefficient</strong> $r$, corrects the above omissions. The coefficient
          takes into account the pairwise behavior of the variables at an observation level $(x_i, y_i)$
          and reintroduces the direction of the relationships.</p>
      </section>

      <!-- correlation  -->
      <section>
        <h2 id="correlation" class="scroll-mt-16 md:scroll-mt-10">Pearson's Correlation</h2>
        <p>Pearson's correlation coefficient, assists the SD-Line with a direction of the relationship and
          it does so by examining the joint behavior of the variables at the level of individual data
          points. </p>
        <p>The correlation coefficient is defined as:<br />
          $$r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i -
          \bar{x})^{2} \sum_{i=1}^{n}(y_i - \bar{y})^{2}}} $$<br />
          You can ignore the denominator, as it is simply a standardizing factor and focus on the
          numerator. In the numerator the deviations in both variables are multiplied, measuring the
          strength and direction of the joint behavior for each point. </p>
        <p>Let's say we have an observation ($x_{i}, y_i$) that is to the left ($x_i&lt;\bar{x}$) and above the
          point of averages ($y_i>\bar{y}$). We would be multiplying a negative and a positive value $(x_i
          - \bar{x})(y_i - \bar{y})$ to get a negative value. Doing this for each observation results in a
          number between $[-1, +1]$ thanks to the denominator. </p>


        <figure class="flex justify-center items-center">
          <div>
            <img src="img/scatter_reg/correlation_quadrants.png" alt="Correlation quadrants">
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

          </figcaption>
        </figure>

        <p>The sign of the coefficient indicates where the majority of the observations lie from the point
          of the averages. </p>
        <p>Observations in the lower-left and upper-right quadrants, result in positive terms in the
          numerator. Conversely, observations in the upper-left and lower-right quadrants result in
          negative terms. Overall the remaining sign tells us the general direction of the cloud of
          observations: positive if more observations were in the lower-left and upper-right quadrants,
          negative when otherwise. The standardized magnitude of the value indicates the strength of the
          linear relationship between $y$ and $x$ or how tightly or loosely the sample points are
          clustered. </p>
        <p>Much could be said about the Pearson's correlation coefficient $r$ but I will summarize the most
          important points about it.</p>
        <ul>
          <li>$r$ contains information about the direction (sign) and strength (value) of a linear
            relationship between <strong>two</strong> variables.</li>
          <li>A $r\approx1$ indicates a tightly clustered positive, and $r\approx-1$ indicates a tightly
            clustered negative linear relationship of sample points. And $r\approx0$ means that we
            either we have a random cluster of sample points or a non-linear relationship. </li>
          <li>For non-linear relationships or straight horizontal / vertical aligned sample points the
            coefficient fails and is around zero. See in the graph below.</li>
          <li>Therefore, a low correlation coefficient should not be confused with independence of the
            variables. To be on the safe side plot your data and make sure it is monotonic (steadily
            increasing or decreasing), otherwise, the correlation coefficient may not be meaningful.
          </li>
          <figure class="flex justify-center items-center">
            <div>
              <img src="img/scatter_reg/corrcoeff.svg" alt="Correlation coefficients">
            </div>
            <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

            </figcaption>
          </figure>
          <li><strong>Correlation is not causation</strong>. Correlation may be due to pure chance alone
            or to a third, hidden factor (also known as a confounder) that independently affects the
            observed variables. </li>
          <li>The formula for correlation coefficients formula can also be written as: $$r=\frac{s_{x
            y}}{s_xs_{y}}$$where $s_{x y}=\frac{1}{n-1} \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$ is
            the <strong>sample covariance</strong> that measures the joint variability of $x$ and $y$,
            while $s_x$ and $s_{y}$ are the respective standard deviations that normalize the
            covariance. The correlation coefficient can thus be thought of as a normalized version of
            the covariance.</li>
          <li>The correlation coefﬁcient is a unitless number. It is not affected by:<ul>
              <li>Interchanging the two variables.</li>
              <li><u>Changes of scale</u>: Adding the same number or multiplying all its values by the
                same positive number only changes the scale of the variable and does not affect $r$.
              </li>
            </ul>
          </li>
        </ul>
        <p>Most importantly for us however is that the correlation coefficient provides the pieces that our
          SD-Line was missing to become a proper regression model. While the slope of the SD-Line
          ($\frac{s_y}{s_{x}}$) has no sense of direction and does not provide enough information about
          the joint behavior of the variables, the correlation coefficient does. </p>




      </section>

      <!-- regression method  -->
      <section>
        <h2 id="regression_method" class="scroll-mt-16 md:scroll-mt-10">Regression Method</h2>
        <p>By adjusting the slope of the SD Line with the correlation coefficient we obtain the correct
          slope of the regression line.<br />
          $$r*\frac{s_y}{s_{x}}$$<br />
          A line with this adjusted slope changes only by a certain percentage $(r)$ of the 'typical'
          deviations ($\frac{s_y}{s_{x}}$) from the point of averages. And the sign of $r$ remedies the
          stubborn positiveness of the the SD-Line. In the plot below you can adjust the correlation
          coefficient to see what it does to the SD-Line.</p>
        <p>Combining the slope of the SD-Line and the correlation coefficient is called the regression
          method. A line anchored in the point of averages $(\bar{x}, \bar{y})$ with a slope described by
          $r*\frac{s_y}{s_{x}}$ is a full-fledged regression line, but not quite what I promised, namely a
          formula with an intercept, slope and error term.</p>
        <p>You can play with the slider in the plot below to see what the correlation coefficient does to the
          SD-Line.</p>

        <!-- correlation coefficient interaction  -->
        <figure class="flex flex-col justify-center items-center object-contain">
          <div>
            <iframe src="./plots/scatter_graphs/correlation_coeff.html" title="correlation interact" width="700"
              height="500" scrolling="no" allowfullscreen loading="lazy" seamless></iframe>
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
            Play with the slider to see how the correlation coefficient $r$ adjusts the slope of the
            SD-Line. The shown line has a: $\text{slope}= r \times \frac{s_y}{s_x}$. If $r=1$ the line
            equals the SD-Line, by moving the slider you tune down the rough assumption of the SD-Line
            &ndash;&nbsp;namely that one step in $s_x$ corresponds to one step in $s_y$&nbsp;&ndash; to be a
            fraction of $\frac{s_y}{s_x}$. An $r=0$ would suggest that $x$ contributes no information for
            the prediction of $y$. The 'best fit' line has $r=0.55$. If the observations were mirrored
            &ndash;&nbsp;going from top left to bottom right&nbsp;&ndash;the correct $r$ would be found in
            the negative.
          </figcaption>
        </figure>

        <p>I have introduced you to the regression method, because it allowed me to explain some integral
          concepts along the way and hopefully gave you an understanding of what regression is all about.
          Anyway, now it’s time to visit the the more common method of deriving at a regression model with
          the well-known formula, the method of least squares. </p>
        <p>With the method of least squares we will take a different approach but the result will be the
          same as the one from the regression method. The rearrangement of $r*\frac{s_y}{s_{x}}$ into the
          slope $\beta_1$ and the intercept $\beta_0$ will be provided below. </p>
        </div>

      </section>

      <!-- OLS  -->
      <section>
        <h2 id="OLS" class="scroll-mt-16 md:scroll-mt-10">Method of Least Squares</h2>
        <p>This method approaches the problem directly by minimizing the errors $(y_i -\hat{y}_i)$. The method
          is best
          illustrated graphically. In the figure below we see the same dataset twice with different
          straight lines placed between the observations. The goal is to find a line that passes through
          most of the sample points $i$ or at least minimizes the distance between the points (true label
          $y_i)$ and the estimates $\hat{y}_i$ represented by the line. You may find many lines that will
          reduce the <em>sum of errors</em> to zero: $\sum(y_i -\hat{y}_i)=0$, due to the fact that
          positive and negative errors cancel each other out. But it can be shown that there is only one
          line for which the <em>sum of squared errors</em> $\sum(y_i -\hat{y}_i)^2$ is a minimum. </p>
        <p>The smallest possible sum of squared errors, <strong>$SSE$</strong> for short, would be zero,
          since there can be no negative value after squaring. Data with a $SSE=0$ were perfectly aligned
          on a straight line, a deterministic relationship without aberrations, which is obviously
          not a realistic scenario for natural data. </p>

        <figure class="flex justify-center items-center">
          <div class="overflow-hidden transition-transform transform hover:scale-150 border-transparent">
            <img src="img/scatter_reg/SSE.png" alt="Errors of prediction"
              style="object-fit: contain; width: 100%; height: auto;">
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

          </figcaption>
        </figure>

        <p>We're left with $SSE&gt;0$, which we want to minimize. We can describe this mathematically as an
          optimization problem, which allows us to use derivatives to determine optimal parameters for the
          regression formula. </p>
        <p>Here is the regression formula again, this time with little hats to indicate that the symbols
          below them are estimates of their unknown population parameters (population parameters do not
          wear hats).<br />
          $$\hat{y}=\hat\beta_0+\hat\beta_{1}x$$<br />
          All of the estimates are unknown to us at this stage, what we have are the data, $x$ and $y$ and
          what we can infer from them, such as the means $(\bar{x}, \bar{y})$ and standard deviations
          $(s_x, s_y)$. We can formulate the optimization problem in a way that we derive the unknown
          parameters, by minimizing and rearranging the symbols. </p>
        <p>Here is a quick summary of the math:</p>
        <p>The goal is to minimize the sum of squared errors: $\sum(y_{i} - \hat{y_i})^2$, the
          <strong>objective function</strong> to minimize then is: $\sum \left[y_i - (\beta_0 + \beta_1
          x_i)\right]^2$.
        </p>
        <p>A typical optimization notation is $Q$ for the objective function, with the parameter estimates
          to be minimized in parentheses. <br />
          $$\text{Find }\min_{\beta_{0}, \,\beta_1} Q(\beta_{0}, \,\beta_1), \quad\quad \text{for }
          Q(\beta_{0},\,\beta_1) = \sum \left[y_i - (\beta_0 + \beta_1 x_i)\right]^2$$<br />
          Minimizing the objective function for the intercept $\beta_0$ is equivalent to finding the best
          vertical position of the line.<br />
          $$\frac{\partial Q(\beta_{0}, \beta_1)}{\beta_{0}}\quad \rightarrow \quad
          \hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$<br />
          Minimizing the objective function for $\beta_1$​ ensures that the regression line has the
          optimal slope to best capture or relationship between the variables.<br />
          $$\frac{\partial Q(\beta_{0}, \beta_1)}{\beta_{1}}\quad \rightarrow \quad \hat{\beta_1}=
          \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^{2}} = \frac{s_{x y}}{s_x^2}$$</p>
        <p>$\hat{\beta_1}$ is now defined by terms we already know. <br />
          $\hat\beta_{0}$ is found by implanting $\hat{\beta_1}$ in the first derivative
          $\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$. </p>
        <p>$\hat{\beta_0}, \hat{\beta_1}$ are called the <strong>least squares estimates</strong> of the
          population parameters $\beta_0$ and $\beta_1$, and the prediction equation
          $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ is called the <strong>least squares line</strong> or line
          of best fit. </p>
        <p>The least squares line $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ has the following two properties:</p>
        <ol>
          <li>The sum of the errors equals 0 , i.e., mean error = 0.<br />
            $$\sum ε_i =0 \quad \&amp; \quad \bar ε =0$$</li>
          <li>The $SSE$ is smaller than for any other straight-line model, i.e.,
            the error variance is minimal.</li>
        </ol>
        <p>The first point explains why we don’t see the error term $ε$ in the least squares line equation,
          while there is still uncertainty in the predictions the error is implicitly accounted for in the
          estimated parameters.</p>
        <p>I have said before that this slope of $\hat\beta_1$ is the same as the one found out by combining
          the slope of the SD-Line and the correlation coefficient. Here's how they are related in
          symbols:<br />
          $$\hat\beta_1=r \frac{s_y}{s_{x}}=\frac{s_{x y}}{s_{x}\thinspace s_{y}}\,\frac{s_y}{s_{x}}
          =\frac{s_{x y}}{s_{x}^2}$$<br />
          where $r=\frac{s_{x y}}{s_xs_{y}}$ is the correlation coefficient, $s_x$ and $s_y$ are the
          sample standard deviations, $s_{xy}$ the sample covariance and $s_x^2$ is the sample variance of
          $x$. </p>
        <p>How do we now find the intersection of our line with the y-axis? Looking at the formula we
          arrived at, we can understand it as walking from the point of averages $(\bar{x}, \bar{y})$
          along the slope for the length of one $\bar{x}$ towards the y-axis. We will end up somewhere
          above or beyond $\bar{y}$, depending on the sign of $\beta_1$, as long as our slope is not zero.
          <br />
          $$\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$<br />
          $\hat\beta_0$ represents the predicted value of $y$ when $x = 0$. Note, however, that this value
          is meaningless if $x = 0$ is nonsensical or outside the range of the sample data.
        </p>

        <figure class="flex justify-center items-center">
          <div>
            <img src="img/scatter_reg/intercept.png" alt="Intercept" style="width: 700px; height: 500px;">
          </div>
          <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

          </figcaption>
        </figure>
        <p>The least square regression model: $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ embodies Galton’s
          discoveries, that children tend to be taller when their parents are tall, and that observations
          tend to return to their mean. Galton’s findings are reflected in the estimated parameters as the
          parameters where constructed by using the point of averages as a reference and describing the
          deviations of the observations from this point. </p>
      </section>

      <!-- Interpretation -->
      <section>
        <h2 id="interpretation" class="scroll-mt-16 md:scroll-mt-10">Interpretation</h2>
        <p>Interpreting the estimated parameters depends on the units of our variables. If we are inferring
          weight (pounds) from height (inches), we would say for every one-inch increase in height our
          model suggests a weight increases by $\beta_1$ pounds. </p>

        <div>
          <img src="img/scatter_reg/weight-height_intercept.png" alt="Interpretation Intercept"
            style="max-width:300px; float:right; margin:16px 0 4px 20px;">
        </div>
        <div>
          <p>The intercept $\beta_0$ is more obscure. It represents the value of the response variable
            when the explanatory variable is zero. In the height and weight example, $\beta_0$
            represents the weight of someone who is zero inches tall. The regression equation in the
            graph on the right is $\hat{y}=-106+3.43x$, from the formula alone one would say "a person
            of zero inches has a weight of $-106$ pounds", a very peculiar person indeed. Obviously, the
            intercept makes no sense if our explanatory variable is unreasonable.</p>
          <p>When we talked about predictions in the terminology chapter, I advised against straying too
            far from the available data to make predictions outside of the range of values you actually
            observed. Trying to find the weight of a person who is zero inches tall should explain why
            this it is not a good idea. The intercept is mostly a mathematical tool for drawing a
            line. Estimates of $\hat{y}$ are most plausible around the center of your observations,
            beyond the range of observed values the meaningfulness of the estimates diminishes quickly.
          </p>
        </div>
        <p>Looking again at the concrete example of $\hat{y}=-106+3.43x$, we can say that with each
          additional inch of height, a person's weight increases by $3.43$ pounds, on average. As long as
          we stay close to our observed range of values for the explanatory variable, in this case between
          $63.43$ and $73.90$ inches.</p>
      </section>

      <!-- summary  -->
      <section>
        <h2 id="summary" class="scroll-mt-16 md:scroll-mt-10">Summary</h2>
        <p>We have all our pieces together for a regression model that is ready to predict an event by
          providing the model with an input. Let’s recapture the essentials. </p>
        <ul>
          <li>A simple linear regression quantifies the effect of an explanatory variable on a response
            variable, assuming a linear relationship between them.</li>
          <li>The problem can be stated as an optimization problem, with the goal of minimizing the sum of
            squared errors $(SSE)$ between true labels and estimates. $$\min_{\beta_{0}, \,\beta_1} \,(SSE)
            =
            \min_{\beta_{0}, \,\beta_1} \left\{\sum (y_{i} - \hat{y}_i)^2 \right\} = \min_{\beta_{0},
            \,\beta_1} \left\{\sum[y_i-(\hat\beta_0+\hat\beta_{1}x_i)]^2\right\}$$
          </li>
          <li>The resulting parameter estimates: $\hat\beta_{0}, \hat\beta_{1}$ are composed of sample
            statistics like the means $(\bar{x}, \bar{y})$, the standard deviations $(s_x, s_{y})$ and
            the joint variation summarized in the sample covariance $(s_{x y})$.<br />
            $$\hat{\beta_1}= \frac{s_{x y}}{s_x^2}$$<br />
            $$\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$</li>
          <li>A line is found by examining the the variability of the data points around their means,
            where the point of averages $(\bar{x}, \bar{y})$ serves as a reference for the examination.
          </li>
          <li>The final outcome is a simple (two variable relationship) linear regression model concretely
            implemented by the least square line: $$\hat{y}=\hat\beta_0+\hat\beta_{1}x$$</li>
        </ul>
      </section>

      <!-- sources  -->
      <section class="tracking-tight leading-7">
        <h2 id="sources" class="scroll-mt-16 md:scroll-mt-10">Sources</h2>
        <p>McClave, J. T., Benson, P. G., &amp; Sincich, T. (2018). <em>Statistics for business and
            economics</em> (Thirteenth edition, global edition). Pearson.</p>
        <p>Freedman, D., Pisani, R., &amp; Purves, R. (2007). <em>Statistics</em> (International student
          ed., 4. ed). Norton.</p>
        <p>Witte, R. S., &amp; Witte, J. S. (2017). <em>Statistics</em> (Eleventh edition). Wiley.</p>

      </section>

    </article>
  </main>

  <!-- footer  -->
  <hr class="mt-10 border-gray-200 dark:border-gray-700">
  <footer class="bg-white dark:bg-gray-900">
    <div class="container">

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-5">

        <div class="">
          <!-- Logo and Name  -->
          <div class="hidden sm:block">
            <div class="flex justify-between items-center">
              <div class="flex gap-3 items-center">
                <a>
                  <img id="logo2" class="h-16 fill-current dark:filter-white" src="./img/logo/logo.svg" alt="">
                </a>
                <div class="">
                  <p
                    class="pb-3 text-lg font-literata font-semibold tracking-widest uppercase rounded-lg focus:outline-none focus:shadow-outline">
                    Marco</p>
                  <p
                    class="text-lg font-literata font-semibold tracking-widest uppercase rounded-lg focus:outline-none focus:shadow-outline">
                    Zausch</p>
                </div>
              </div>
            </div>
          </div>


        </div>

        <div class="flex flex-col sm:flex-row sm:gap-24 justify-between">

          <!-- CONTACT ME -->
          <div class="py-3 md:py-0 flex-1">
            <div class="text-sm uppercase text-gray-500 dark:text-gray-400 font-medium">Contact Me</div>
            <a id="mlink" class="flex items-center gap-3 my-3 hover:text-indigo-400 dark:hover:text-sky-300"
              href="#">Email</a>
          </div>

          <!-- WORK -->
          <div class="py-3 md:py-0 flex-1">
            <div class="text-sm uppercase text-gray-500 dark:text-gray-400 font-medium">Work</div>
            <p class="my-3 flex items-center gap-3">
              <a href="https://www.linkedin.com/in/marcozausch" aria-label="LinkedIn">
                <span
                  class="icon-[fa6-brands--linkedin] fill-current w-5 h-5 text-gray-500 dark:text-gray-300 hover:text-indigo-400 dark:hover:text-sky-300"></span>
              </a>
              <a class="block hover:text-indigo-400 dark:hover:text-sky-300"
                href="https://www.linkedin.com/in/marcozausch">LinkedIn</a>
            </p>
            <p class="my-3 flex items-center gap-3">
              <a href="https://github.com/MaCoZu" aria-label="Github">
                <span
                  class="icon-[fa6-brands--github] fill-current w-5 h-5 text-gray-500 dark:text-gray-300 hover:text-indigo-400 dark:hover:text-sky-300"></span>
              </a>
              <a class="block hover:text-indigo-400 dark:hover:text-sky-300" href="https://github.com/MaCoZu">Github</a>
            </p>
          </div>

          <!-- LEGAL -->
          <div class="py-3 md:py-0 flex-1">
            <div class="text-sm uppercase font-medium text-gray-500 dark:text-gray-400">Legal</div>
            <a class="my-3 block hover:text-indigo-400 dark:hover:text-sky-300" href="./impressum.html">Impressum</a>
            <a class="my-3 block hover:text-indigo-400 dark:hover:text-sky-300" href="./privacypolicy.html">Privacy
              Policy</a>
          </div>
        </div>
      </div>

    </div>
  </footer>

  <script>
    /* 1. define variables */
    var me = "marcozausch";
    var place = "posteo.de";

    /* 2. find email link to replace */
    var elink = document.getElementById("mlink");

    /* 3. replace link href with variables  */
    elink.href = `mailto:${me}@${place}`;

    const drawerToggle = document.getElementById('drawer-toggle');
    const sidebar = document.getElementById('sidebar_model');
    const drawerClose = document.getElementById('drawer-close');
    let timeoutId;

    // Open drawer on hover over the toggle
    drawerToggle.addEventListener('mouseenter', () => {
      clearTimeout(timeoutId);
      sidebar.classList.remove('translate-x-full');
    });

    // Close drawer on mouse leave with delay
    sidebar.addEventListener('mouseleave', () => {
      timeoutId = setTimeout(() => {
        sidebar.classList.add('translate-x-full');
      }, 300); // Delay before closing
    });

    // Close drawer when clicking X on sidebar
    drawerClose.addEventListener('click', () => {
      sidebar.classList.add('translate-x-full');
    });

    // Close the drawer when clicking outside (on the main content)
    document.addEventListener('click', (e) => {
      if (!sidebar.contains(e.target) && !drawerToggle.contains(e.target)) {
        sidebar.classList.add('translate-x-full');
      }
    });

    // Close the drawer when clicking on sidebar links
    document.querySelectorAll('#sidebar_model a').forEach((link) => {
      link.addEventListener('click', () => {
        sidebar.classList.add('translate-x-full');
      });
    });

    // Close the drawer when interacting with iframes
    document.querySelectorAll('iframe').forEach((iframe) => {
      iframe.addEventListener('load', () => {
        iframe.contentWindow.addEventListener('click', () => {
          sidebar.classList.add('translate-x-full');
        });
      });
    });



    function resizeIframe(iframe) {
      iframe.style.height = iframe.contentWindow.document.body.scrollHeight + 'px';
    }
  </script>

  <script type="module" src="../main.js"></script>
  <script type="module" src="../script.js"></script>
</body>

</html>