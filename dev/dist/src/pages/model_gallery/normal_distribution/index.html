<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Normal Distribution</title>

    <!-- Favicon -->
    <link rel="icon" href="../../../../img/logo/logo_weiß.svg" type="image/x-icon" />

    <!-- Styles -->
  <script type="module" crossorigin src="../../../../assets/model_gallery/normal_distribution/index-oJQ1vh6B.js"></script>
  <link rel="modulepreload" crossorigin href="../../../../assets/loadComponents-IR4gVqQb.js">
  <link rel="modulepreload" crossorigin href="../../../../assets/transform-D0hhaYzD.js">
  <link rel="modulepreload" crossorigin href="../../../../assets/init-BFKUnIhM.js">
  <link rel="modulepreload" crossorigin href="../../../../assets/axis-d8_a1sG8.js">
  <link rel="modulepreload" crossorigin href="../../../../assets/line-CJFRD-Fx.js">
  <link rel="stylesheet" crossorigin href="../../../../assets/loadComponents-BpMYHXdy.css">
  <link rel="stylesheet" crossorigin href="../../../../assets/index-CTrXdZ0S.css">
</head>

<body class="font-work mx-6 md:mx-0 bg-white dark:bg-gray-900 text-gray-700 dark:text-gray-200">

    <div id="navbar"></div>
    <div id="model_drawer"></div>

    <main id="main-content"
        class="container mx-auto flex flex-col justify-center items-center mt-8 min-h-screen font-literata divide-y-2 divide-solid prose prose-stone prose-lg md:prose-xl text-pretty leading-8 md:leading-10 dark:prose-invert">

        <!-- normal distribution  -->
        <div id="std-norm" class="scroll-mt-10 md:-scroll-mt-1 z-0">
            <h2 class="mt-8 md:mt-10 text-center font-semibold tracking-wider">Normal
                Distribution</h2>

            <!-- section 1 -->
            <div class="py-0 overflow-visible">

                <!-- text  -->
                <div class="">
                    <p>All kinds of variables in natural and social sciences are (approximately) normally distributed.
                        Height, birth weight, reading ability, job satisfaction, or test scores are just a few examples
                        of such variables. Because normally distributed variables are so common, many statistical tests
                        are designed around them. </p>
                    <p>Normal distributions can be applied even to variables that are not normally distributed, with the
                        help of the <strong>central limit theorem (CLT).</strong> The CLT says that when you repeatedly
                        sample from independent and identically distributed (other than normally distributed) random
                        variables, and take the arithmetic mean of each of those sample, these means will create a
                        distribution that closely approximates a normal distribution. Thereby statistical methods that
                        work for normal distributions can be applied to many problems involving other types of
                        distributions. </p>
                    <p>In a normal distribution the observations are clustered around the mean and become sparse as they
                        move away from the center. This leads to the typical bell-shaped curve. The normal distribution
                        is often identified as the bell curve but there is other bell-shaped distributions. Assuming
                        that the observations fall at the same rate above and below the mean the normal distribution is
                        symmetric which yields convenient characteristics. For example the measures of central tendency
                        – mean ($\mu$), median, and mode – fall in one place. This place splits the dataset cleanly in
                        the middle where 50% of the values fall below and the other half above the mean. </p>

                </div>

                <!-- figure  -->
                <div class="flex-col justify-center items-center relative h-fit overflow-scroll">
                    <!-- Sliders -->
                    <div class="grid grid-cols-2">
                        <label for="mean_nd">Mean (μ): <span id="mean-value_nd">0</span></label>
                        <input type="range" id="mean_nd" min="-5" max="5" value="0" step="0.1">
                    </div>
                    <div class="grid grid-cols-2">
                        <label for="std_nd">Standard Deviation (σ): <span id="std_value_nd">1</span></label>
                        <input type="range" id="std_nd" min="0.1" max="5" value="1" step="0.1">
                    </div>

                    <!-- Chart Container -->
                    <div class="my-4" id="normal_distribution_chart"></div>
                </div>
            </div>

            <div class="max-w-full md:max-w-xl lg:max-w-2xl xl:max-w-3xl mx-auto">
                <p>The distribution can be described by two values: the mean and the standard deviation. While the mean
                    determines the location of the normal distribution on the $x$-axis the standard deviation defines
                    its shape. The standard deviation $\sigma$ can be thought of as a “typical” distance of the
                    observations $x_i$ from their mean, $\mu$. Higher standard deviations make for flatter curves, try
                    it by moving the slider of the graph to a value of $\sigma&gt;1$. </p>
                <p>A further very practical feature of normal distributions is called the empirical rule (aka.
                    $\mathbf{68-95-99.7}$ rule) and it states, that 68% of the observations lie within one standard
                    deviation ($\pm \, \sigma$) around the mean, 95% of observation within $\pm \, 2\sigma$, and 97%
                    within $\pm \, 3\sigma$. The empirical rule is not affected by different values of standard
                    deviation, that is to say that a flat normal distribution ($\sigma&gt;1$) and a peaked normal
                    distribution ($\sigma&lt;1$) both adhere to the rule. Say, you have a normal distribution with
                    $\mu=100$ and $\sigma=15$ than you can immediately tell that 95% of you observations will be found
                    in the range of $[70, 130]$ or $[100 - 2\times15, 100 + 2\times15]$. </p>
                <p>In business companies try to reduce the deviation from a desired property to a minimum. For example
                    their goal may be to improve their production or business process so that any deviation from the
                    perfect quality becomes negligibly small. Motorola coined the <strong>Six Sigma</strong> method,
                    where rigorous quality control reduces aberrations to such a minuscule $\sigma$, that even output
                    that strays six standard deviations ($\pm\,6\sigma$) away from the optimum quality, can be rendered
                    good enough. In other words: the production process becomes so precise, that six sigma's of
                    deviation are not a problem.</p>
                <p>The usual application of the normal distribution can be summarized in the question: “How likely is it
                    that I will see an observation smaller, equal to or greater than some value $x$?” To simplify things
                    one often standardizes the $x$ in question into the corresponding $z$-value, by subtracting the mean
                    and dividing with the standard deviation. This makes it easy to look up the probability in a
                    prepared table for the standard normal distribution.<br /> $$z=(\frac{{x - \mu}}{{\sigma}})$$<br />
                    The standard normal distribution is the normal distribution with the parameters $\mu=0$ and
                    $\sigma=1$. Standardization is handy because any normal distribution can be transformed into a
                    standard normal distribution. In fact every normal distribution is a version of the standard normal
                    distribution, whose domain has been stretched by $\sigma$ and shifted by $\mu$. Thus, any $x$-value
                    from one normal distributions, can be transformed in its corresponding $z$-value and be checked on
                    the same table as any other $x$-value coming from a different normal distribution (with different
                    $\mu$ and $\sigma$ parameters).</p>
                <p>By playing with the graph above your essentially transforming the $x$-values of the curve through the
                    parameters, if you put the sliders to $\mu=0$ and $\sigma=1$, you’ll see the standard normal
                    distribution. The transition from some other shape of the curve to this one can be understood as the
                    standardization of all the $x$-values. </p>
            </div>


            <!-- Section 2 -->
            <div class="grid grid-cols-1 md:grid-cols-3 gap-4 md:gap-8 items-start md:px-0 md:mt-0 h-fit">

                <!-- Text Content -->
                <div class="md:col-span-2 max-w-none mx-auto md:mx-0">
                    <p>
                        For example if we have a data set with a mean $\mu_x = 5.0$ and standard deviation
                        $\sigma_x=3.09$, $z$-scores are as
                        follows:
                    </p>

                    <p>
                        Each data point $x$ is now expressed as the difference from $\mu_x$ in units of the standard
                        deviation $\sigma_x$, this
                        is the $z$-score.
                    </p>
                    <p>Each $z$-score is associated with a probability, or $\mathbf{p}$<strong>-value</strong>, that
                        tells you the cumulative probability of values occurring below that $z$-score. That is the
                        percentage of observations that will likely occur up to this point. You could for example ask,
                        how likely is it, that I will see an observation equal to or below 8:<br />
                        $P(x\leq8)=P(z\leq0.97)=0.83\%$. </p>
                </div>

                <!-- Table -->
                <div class="md:col-span-1 overflow-x-auto">
                    <table id="cssTable" class="w-full border-separate border-spacing-1 border border-gray-300">
                        <thead>
                            <tr class="bg-gray-100">
                                <th class="text-center border border-gray-300 px-4 py-2">x</th>
                                <th class="text-center border border-gray-300 px-4 py-2">z</th>
                                <th class="text-center border border-gray-300 px-4 py-2">p-value</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0</td>
                                <td>-1.62</td>
                                <td>0.053</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>-0.97</td>
                                <td>0.166</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>-0.97</td>
                                <td>0.166</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>0</td>
                                <td>0.5</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>0</td>
                                <td>0.5</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>0</td>
                                <td>0.5</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>0.97</td>
                                <td>0.83</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>0.97</td>
                                <td>0.83</td>
                            </tr>
                            <tr>
                                <td>10</td>
                                <td>1.62</td>
                                <td>0.95</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="col-span-3">
                    <p>With the p-values you can also determine how likely it is, that random variable in the
                        distribution is greater than 10. Since the $p$-value typically gives the probability of values
                        occurring below the $x$ in question, you will find the probability of values occurring above
                        this point by subtracting the $p$-value from 1:<br /> $P(x\leq10)=P(z\leq1.62) =0.95\%
                        \rightarrow (1-0.95)=0.05\%$. </p>
                    <p>If you ask for the probability of some $x$ occurring in a certain range of values, you subtract
                        the smaller cumulative probability from the greater cumulative probability. Since the greater
                        probability already includes the case of the smaller one. Here the probability of $x&lt;10$
                        already includes the case of $x&lt;2$, if we wanted to know the probability of $2&lt;x&lt;10$
                        you have to remove the smaller probability from the greater. <br /> $P(2&lt;x&lt;10) = (0.95 -
                        0.166)=0.78\%$.</p>
                    <p>If you had mutually exclusive cases you would add the probabilities together.<br />
                        $P(2&lt;x)\,or\,P(x&gt;10) = 0.166 + 0.05 =0.174\%$.</p>
                </div>

            </div>

        </div>



    </main>


    <div id="footer"></div>


    <!-- load components  -->

    <!-- normal distribution chart -->

</body>

</html>